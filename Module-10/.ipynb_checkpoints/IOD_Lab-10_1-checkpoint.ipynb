{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fix0Ry0j1qn5"
   },
   "source": [
    "<div>\n",
    "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U7WTDbVT1qn9"
   },
   "source": [
    "# Lab 10.1: Introduction to AWS SageMaker and Managed Services\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Introduction\n",
    "In this lab you will be introduced to some Amazon Web Services with access via an AWS Educate Starter account. Notably, you will see how to work with Jupyter notebooks in a SageMaker session, make use of cloud storage through an S3 bucket and explore several demos related to image recognition and text analysis.\n",
    "\n",
    "Note that this notebook will need to be loaded into an AWS SageMaker instance under a `conda_python3` environment in order to be run succesfully.\n",
    "\n",
    "At a later date after signing into the console you may navigate to https://console.aws.amazon.com/cost-management/home, then select \"Explore Costs\" then \"Service\" to see a breakdown of costs by AWS service for recent days. This will ensure that you are not still being charged for any services that were unintentionally not terminated after this module's labs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Connecting to AWS via AWS Educate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS Educate provides IOD students with access to $30 credits of access to many of Amazon's Web Services without requiring credit card details. The course coordinator will have provided you access to AWS Educate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Accept the invitation to AWS Educate using the link provided in your email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Sign in to https://www.awseducate.com and navigate to the \"Classrooms & Credits\" tab, then \"Request or go to an AWS Educate Classroom\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Select \"My classrooms\" and then the \"Go to classroom\" link that appears under \"Courses where I am a Student\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) You will be redirected to a https://labs.vocareum.com page. Vocareum is an external provider that maintains the provisioning of AWS Educate Starter accounts. You should see \"Welcome to your AWS Educate Account\" where you can see your remaining credits (initialised to $30) and remaining session time of 2:60 (3 hours which can be extended). \n",
    "\n",
    "If the 3 hours runs out, simply reload the Workbench page and click again on the AWS Console button."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Click \"AWS Console\" and you will be directed to your AWS console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Spend a few moments familiarising yourself with this page noting the extensive list of AWS services within categories such as Compute, Machine Learning, Storage and Analytics. All of these are documented at https://docs.aws.amazon.com/. Note that the $30 credit starts to be consumed when you start up services such as S3 (storage) and SageMaker (Jupyter notebooks) and not by simply browing the console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating an Amazon SageMaker notebook instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) From the console select \"Amazon SageMaker\". From the left select \"Notebook instances\". https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/notebook-instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Click the orange button \"Create notebook instance\" and enter a Notebook instance name of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Keep the notebook instance type as `ml.t2.medium` (this incurs a cost of about 4 cents per hour). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Under \"Permissions and encryption\" - IAM role, select \"create a new IAM role\" if you do not have one already. This is a set of permissions to make AWS service requests. More information about IAM (Identity & Access Management) can be found at https://aws.amazon.com/iam/faqs/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) All other options may be left at their default settings. Note that there is also the option to clone a public git repository to this notebook instance but we will not be doing this here. Finally select \"Create notebook instance\" at the bottom of the page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) It usually takes a few minutes for the instance to be ready as the status changes from \"Pending\" to \"InService\". Then you may click on \"Open Jupyter\" and you will be taken to a familiar jupyter dashboard view. You may browse through a number of SageMaker example notebooks by selecting the \"SageMaker Examples\" tab. Any of these can either be previewed or used. Note however that training machine learning models requires additional permissions beyond what AWS Educate provides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using SageMaker and S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload this notebook into your AWS SageMaker instance under a `conda_python3` environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ee2ul0ED1qoD"
   },
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-pCdtFw1qoF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from skimage import io\n",
    "from io import BytesIO\n",
    "import urllib.request\n",
    "from matplotlib import pyplot as plt\n",
    "import boto3 #AWS API\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boto3 library allows access to AWS APIs via wrapper functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clients provide low-level access to Amazon Web Services. Resources are higher level with more object-oriented readable syntax providing less functionality than clients. Examples of resources include or S3 buckets (storage) or EC2 instances (computing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "s3_resource = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the cell below should confirm that your SageMaker instance is in the us-east-1 region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_region = boto3.session.Session().region_name \n",
    "print(\"Success - my SageMaker instance is in the \" + my_region + \" region.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create an S3 bucket (container) into which we will place data. Files within buckets are accessed by keys. Hence note that while dir1/f1 and dir1/f2 are keys referencing two files, f1 and f2 are not interpreted as files contained within dir1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bucket_name = 'your-bucket-name' # <--- CHANGE THIS TO A UNIQUE NAME FOR YOUR BUCKET (no uppercase letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if  my_region == 'us-east-1':\n",
    "        s3_resource.create_bucket(Bucket=my_bucket_name)\n",
    "    else: \n",
    "        s3_resource.create_bucket(Bucket=my_bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\n",
    "    print('S3 bucket created successfully')\n",
    "except Exception as e:\n",
    "    print('S3 error: ',e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that a bucket was created also by viewing the S3 console at https://console.aws.amazon.com/s3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Fill in the missing code to list your S3 buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ???\n",
    "\n",
    "# Output the bucket names\n",
    "print('Existing buckets:')\n",
    "for bucket in ???:\n",
    "    print(f'  {bucket[\"Name\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code allows one to list the contents of a bucket along with file sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function from https://github.com/aws-samples/aws-open-data-analytics-notebooks/blob/master/exploring-data/README.md\n",
    "\n",
    "def list_bucket_contents(bucket, match='', size_mb=0):\n",
    "    bucket_resource = s3_resource.Bucket(bucket)\n",
    "    total_size_gb = 0\n",
    "    total_files = 0\n",
    "    match_size_gb = 0\n",
    "    match_files = 0\n",
    "    for key in bucket_resource.objects.all():\n",
    "        key_size_mb = key.size/1024/1024\n",
    "        total_size_gb += key_size_mb\n",
    "        total_files += 1\n",
    "        list_check = False\n",
    "        if not match:\n",
    "            list_check = True\n",
    "        elif match in key.key:\n",
    "            list_check = True\n",
    "        if list_check and not size_mb:\n",
    "            match_files += 1\n",
    "            match_size_gb += key_size_mb\n",
    "            print(f'{key.key} ({key_size_mb:3.0f}MB)')\n",
    "        elif list_check and key_size_mb <= size_mb:\n",
    "            match_files += 1\n",
    "            match_size_gb += key_size_mb\n",
    "            print(f'{key.key} ({key_size_mb:3.0f}MB)')\n",
    "\n",
    "    if match:\n",
    "        print(f'Matched file size is {match_size_gb/1024:3.1f}GB with {match_files} files')            \n",
    "    \n",
    "    print(f'Bucket {bucket} total size is {total_size_gb/1024:3.1f}GB with {total_files} files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will investigate the contents of a bucket from the Registry of Open Data on AWS. A listing of these datasets is at https://registry.opendata.aws/. In particular we shall make use of the COCO image dataset (Common Objects in Context), used for object detection and segmentation. From https://registry.opendata.aws/fast-ai-coco/ we see that the bucket name is `fast-ai-coco`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_bucket_contents(bucket='fast-ai-coco')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we copy the `val2017.zip` file (containing 5000 images) to the bucket you created above via the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_to_bucket(origin_bucket_name, destination_bucket_name, file_name):\n",
    "    copy_source = {\n",
    "        'Bucket': origin_bucket_name,\n",
    "        'Key': file_name\n",
    "    }\n",
    "    s3_resource.Object(destination_bucket_name, file_name).copy(copy_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Use the above function to copy val2017.zip to your bucket. Then use the list_bucket_contents function to verify that the 'val2017.zip' was successfully transferred. Alternatively, use the S3 console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S3 is a storage location that does not allow computation. The following code allows us to extract selected files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_obj = s3_resource.Object(bucket_name= my_bucket_name, key='val2017.zip')\n",
    "buffer = BytesIO(zip_obj.get()[\"Body\"].read())\n",
    "z = ZipFile(buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of the files contained in the zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in z.namelist():\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select a sample of these image files for transfer to our SageMaker instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_files = ['007108', '008021', '023781', '024021', '052412', '060855', '062808', '071938', '104424', '108495',\n",
    "              '138115', '170474', '246308', '280891', '324258', '325483', '336232', '402765', '494913']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in z.namelist():\n",
    "    if filename[-10:-4] in list_of_files:\n",
    "        file_info = z.getinfo(filename)\n",
    "        s3_resource.meta.client.upload_fileobj(\n",
    "            z.open(filename),\n",
    "            Bucket=my_bucket_name,\n",
    "            Key=f'{filename}'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Verify that contents of your S3 bucket have been modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we copy the jpg files from your S3 bucket to the local instance for viewing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in list_of_files:\n",
    "    new_file_name =  f + '.jpg'\n",
    "    bucket_file_name = 'val2017/000000' + new_file_name\n",
    "    s3.download_file(my_bucket_name, bucket_file_name, new_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly the ``upload_file`` method can be used to upload local files to the bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Use the imshow and imread methods to plot some or all the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command can be used to save the files in one archive file. This makes it more convenient to download the images at once. Alternatively one can download the entire (5000-file) archive at https://s3.amazonaws.com/fast-ai-coco/val2017.zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save file to local machine\n",
    "!tar chvfz photos.tar.gz *.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with files from a url or your local machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with your Jupyter notebooks that run locally on your machine, SageMaker notebooks can also work with files from the internet or your local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Read in a data file from a url and display the first few lines to verify it loaded successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Upload a file from your DATA folder used in this course. Display the first five rows to verify that it was loaded successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that your free AWS Educate Starter account gives you the ability to explore datasets, but not to train or deploy models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Amazon Rekognition via the Console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go the Amazon Rekognition console page by searching for Rekogntion, or go directly to https://console.aws.amazon.com/rekognition/home?region=us-east-1#/. From there click on \"Try Demo\" to see a series of image recognition capabilities ranging from \"Object and Scene Detection\" to \"Text in Image\". In addition to the samples there you may wish to try either your own examples or the following files that we downloaded above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for Object and Scene Detection: 007108, 008021, 023781, 024021, 052412, 060855, 062808, 071938, 246308, 280891, 336232, 494913 \n",
    "- for Facial Analysis: 104424, 108495, 138115\n",
    "- for Celebrity Recognition: 170474\n",
    "- for Face Comparison: 324258, 325483\n",
    "- for Text in Image: 402765"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing these actions via the API rather than the console requires additional access, such as an Amazon Free Tier account (which requires a credit card)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Amazon Textract via the Console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through the following 10-minute demo to understand the features and capabilities of Amazon Textract to extract text from documents.\n",
    "\n",
    "https://aws.amazon.com/getting-started/hands-on/extract-text-with-amazon-textract/\n",
    "\n",
    "In addition to the same data provided, you may wish to use the files `amzn_stock.pdf` and `sample_form.JPG` from the DATA folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Amazon Comprehend via the Console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through the following 10-minute demo to understand the features and capabilities of Amazon Comprehend for text analysis.\n",
    "\n",
    "https://aws.amazon.com/getting-started/hands-on/analyze-sentiment-comprehend/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Closing your notebook instance and deleting your S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to delete your bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_to_delete = s3_resource.Bucket(my_bucket_name)\n",
    "bucket_to_delete.objects.all().delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the bucket has been emptied by navigating to https://console.aws.amazon.com/s3/. You may then delete the bucket by selecting it in the S3 console page and clicking \"Delete\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download this notebook to your local machine when you are done. Then stop your notebook instance by visiting https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/notebook-instances, selecting the instance and then choosing \"Stop\" under the \"Actions\" menu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RERADKgNFq9T"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> > > > > > > > > Â© 2021 Institute of Data\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DSIA Lab-10_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
